{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be170ba",
   "metadata": {},
   "source": [
    "# Email Detector\n",
    "\n",
    "This notebook demonstrates a sound approach to detect disposable and suspicious email domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9e34f",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Before running this notebook, ensure you have all the required packages installed. You can install them using either pip or conda:\n",
    "\n",
    "### Using pip:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Using conda:\n",
    "```bash\n",
    "conda env update -f environment.yml\n",
    "conda activate email_detector\n",
    "```\n",
    "\n",
    "Or simply run the provided setup script:\n",
    "```bash\n",
    "bash setup_env.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca8ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if required packages are installed\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "required_packages = [\n",
    "    \"pandas\", \"numpy\", \"sklearn\", \"dns.resolver\", \"tqdm\", \"matplotlib\", \n",
    "    \"scipy\", \"whois\"\n",
    "]\n",
    "\n",
    "missing_packages = []\n",
    "for package in required_packages:\n",
    "    pkg_name = package.split('.')[0]  # Get the base package name\n",
    "    if importlib.util.find_spec(pkg_name) is None:\n",
    "        missing_packages.append(pkg_name)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"Warning: Missing packages: {', '.join(missing_packages)}\")\n",
    "    print(\"Please install them with: pip install \" + \" \".join(missing_packages))\n",
    "    \n",
    "# Continue with imports if no errors\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import dns.resolver\n",
    "import whois\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import difflib\n",
    "import ssl\n",
    "import socket\n",
    "import warnings\n",
    "import urllib.parse\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import math\n",
    "import csv\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EmailDetector:\n",
    "    \"\"\"\n",
    "    A ly sound email detector with proper feature engineering,\n",
    "    balanced training, and unbiased predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_path=\"email_detector_config.json\"):\n",
    "        \"\"\"Initialize the detector with  foundations.\"\"\"\n",
    "        self.model = None\n",
    "        self.feature_cache = {}\n",
    "        self.dns_cache = {}\n",
    "\n",
    "        # Set up logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Load configuration\n",
    "        self.config = self._load_config(config_path)\n",
    "\n",
    "        # File paths\n",
    "        self.allow_list_path = self.config.get('allow_list_path', '/input/legitimate_emails.txt')\n",
    "        self.deny_list_path = self.config.get('deny_list_path', '/input/disposable_emails.txt')\n",
    "        self.model_path = self.config.get('model_path', '/models/ikick_email_detector.pkl')\n",
    "        \n",
    "        # Path for suspected disposable domains CSV\n",
    "        self.suspected_disposable_path = self.config.get('suspected_disposable_path', 'suspected_disposable_domains.csv')\n",
    "        self.suspected_domains_threshold = self.config.get('suspected_domains_threshold', 0.55)  # 85% confidence\n",
    "\n",
    "        # Load domain lists\n",
    "        self.legitimate_domains = self._load_domain_list(self.allow_list_path)\n",
    "        self.disposable_domains = self._load_domain_list(self.deny_list_path)\n",
    "        \n",
    "        # Load previously identified suspected disposable domains\n",
    "        self.suspected_disposable_domains = self._load_suspected_domains()\n",
    "\n",
    "        #  parameters\n",
    "        self.feature_importance_threshold = 0.01  # 1% minimum importance\n",
    "        self.confidence_calibration_factor = 1.5\n",
    "        self.class_balance_ratio = 1.0  # Equal weight for both classes\n",
    "\n",
    "        # Model configuration\n",
    "        self.model_config = None\n",
    "        self.feature_scaler = None\n",
    "        self.feature_selector = None\n",
    "        self.calibrated_model = None\n",
    "\n",
    "        # Feature engineering patterns\n",
    "        self.disposable_patterns = {\n",
    "            'keywords': ['temp', 'disposable', 'throwaway', 'guerrilla', 'mailinator'],\n",
    "            'patterns': [r'\\d{3,}$', r'temp\\d+', r'test\\d+']\n",
    "        }\n",
    "\n",
    "        self.legitimate_patterns = {\n",
    "            'tlds': ['.com', '.org', '.net', '.edu', '.gov'],\n",
    "            'patterns': [r'^[a-z]+\\.[a-z]+$', r'^mail\\.[a-z]+\\.[a-z]+$']\n",
    "        }\n",
    "\n",
    "   #configurations\n",
    "    def _load_config(self, config_path):\n",
    "        \"\"\"Load configuration from JSON file.\"\"\"\n",
    "        default_config = {\n",
    "            'use_dns': True,\n",
    "            'use_whois': True, \n",
    "            'max_features': 30,\n",
    "            'cv_folds': 5,\n",
    "            'random_state': 42,\n",
    "            'suspected_disposable_path': '/output/suspected_disposable_domains.csv',\n",
    "            'suspected_domains_threshold': 0.85\n",
    "        }\n",
    "\n",
    "        if config_path and os.path.exists(config_path):\n",
    "            try:\n",
    "                with open(config_path, 'r') as f:\n",
    "                    config = json.load(f)\n",
    "                default_config.update(config)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error loading config: {str(e)}\")\n",
    "\n",
    "        return default_config\n",
    "\n",
    "    def _load_domain_list(self, file_path):\n",
    "        \"\"\"Load domain list from file.\"\"\"\n",
    "        domains = set()\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip().lower()\n",
    "                        if line and not line.startswith('#'):\n",
    "                            domains.add(line)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return domains\n",
    "\n",
    "    def _load_suspected_domains(self):\n",
    "        \"\"\"Load previously identified suspected disposable domains from CSV.\"\"\"\n",
    "        suspected_domains = set()\n",
    "        if os.path.exists(self.suspected_disposable_path):\n",
    "            try:\n",
    "                df = pd.read_csv(self.suspected_disposable_path)\n",
    "                suspected_domains = set(df['domain'].str.lower())\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error loading suspected domains: {str(e)}\")\n",
    "        return suspected_domains\n",
    "\n",
    "    def _save_suspected_domain(self, domain, confidence, reasons):\n",
    "        \"\"\"Save a newly suspected disposable domain to CSV.\"\"\"\n",
    "        # Create the CSV file if it doesn't exist\n",
    "        if not os.path.exists(self.suspected_disposable_path):\n",
    "            with open(self.suspected_disposable_path, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['domain', 'confidence', 'timestamp', 'reasons', 'top_features'])\n",
    "        \n",
    "        # Check if domain already exists in the CSV\n",
    "        if os.path.exists(self.suspected_disposable_path):\n",
    "            df = pd.read_csv(self.suspected_disposable_path)\n",
    "            if domain.lower() in df['domain'].str.lower().values:\n",
    "                return  \n",
    "        \n",
    "        # Prepare reasons string\n",
    "        reasons_str = '; '.join(reasons)\n",
    "        \n",
    "        # Save to CSV\n",
    "        with open(self.suspected_disposable_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                domain,\n",
    "                f\"{confidence:.3f}\",\n",
    "                datetime.now().isoformat(),\n",
    "                reasons_str,\n",
    "                json.dumps([f['feature'] for f in self.last_top_features[:3]])  # Store top 3 feature names\n",
    "            ])\n",
    "        \n",
    "        # Add to in-memory set\n",
    "        self.suspected_disposable_domains.add(domain.lower())\n",
    "        \n",
    "        self.logger.info(f\"Saved suspected disposable domain: {domain} (confidence: {confidence:.3f})\")\n",
    "\n",
    "    def load_emails_from_file(self, file_path):\n",
    "        \"\"\"Load emails from a text file (one email per line).\"\"\"\n",
    "        emails = []\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            self.logger.error(f\"Email file not found: {file_path}\")\n",
    "            return emails\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    # Skip empty lines and comments\n",
    "                    if line and not line.startswith('#'):\n",
    "                        emails.append(line)\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(emails)} emails from {file_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading emails from file: {str(e)}\")\n",
    "        \n",
    "        return emails\n",
    "\n",
    "    def check_emails_from_file(self, file_path, output_csv=None, output_json=None):\n",
    "        \"\"\"Check emails from a text file and optionally save results.\"\"\"\n",
    "        print(f\"\\nProcessing emails from: {file_path}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Load emails\n",
    "        emails = self.load_emails_from_file(file_path)\n",
    "        \n",
    "        if not emails:\n",
    "            print(\"No emails found in file.\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(emails)} emails to check\")\n",
    "        \n",
    "        # Process emails\n",
    "        results = self.predict(emails)\n",
    "        \n",
    "        # Check if we have any results before attempting to display summary\n",
    "        if not results:\n",
    "            print(\"No results obtained from prediction.\")\n",
    "            return []\n",
    "        \n",
    "        # Display summary\n",
    "        disposable_count = sum(1 for r in results if r['is_disposable'])\n",
    "        legitimate_count = len(results) - disposable_count\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"- Total emails: {len(results)}\")\n",
    "        \n",
    "        # Avoid division by zero when calculating percentages\n",
    "        if len(results) > 0:\n",
    "            print(f\"- Disposable: {disposable_count} ({disposable_count/len(results)*100:.1f}%)\")\n",
    "            print(f\"- Legitimate: {legitimate_count} ({legitimate_count/len(results)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"- Disposable: 0 (0.0%)\")\n",
    "            print(\"- Legitimate: 0 (0.0%)\")\n",
    "        \n",
    "        # Save results if output paths specified\n",
    "        if output_csv and results:\n",
    "            self._save_results_to_csv(results, output_csv)\n",
    "            print(f\"\\nResults saved to CSV: {output_csv}\")\n",
    "        \n",
    "        if output_json and results:\n",
    "            self._save_results_to_json(results, output_json)\n",
    "            print(f\"Results saved to JSON: {output_json}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _save_results_to_csv(self, results, output_path):\n",
    "        \"\"\"Save prediction results to CSV file.\"\"\"\n",
    "        # Prepare data for CSV\n",
    "        csv_data = []\n",
    "        for result in results:\n",
    "            csv_data.append({\n",
    "                'email': result['input'],\n",
    "                'domain': result['domain'],\n",
    "                'is_disposable': result['is_disposable'],\n",
    "                'confidence': result['confidence'],\n",
    "                'probability_legitimate': result['probability_legitimate'],\n",
    "                'probability_disposable': result['probability_disposable'],\n",
    "                'in_training_legitimate': result['in_training_legitimate'],\n",
    "                'in_training_disposable': result['in_training_disposable'],\n",
    "                'in_suspected_disposable': result['in_suspected_disposable'],\n",
    "                'top_feature_1': result['top_features'][0]['feature'] if result['top_features'] else '',\n",
    "                'top_feature_1_value': result['top_features'][0]['value'] if result['top_features'] else '',\n",
    "                'top_feature_2': result['top_features'][1]['feature'] if len(result['top_features']) > 1 else '',\n",
    "                'top_feature_2_value': result['top_features'][1]['value'] if len(result['top_features']) > 1 else '',\n",
    "            })\n",
    "        \n",
    "        # Save to CSV\n",
    "        df = pd.DataFrame(csv_data)\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "    def _save_results_to_json(self, results, output_path):\n",
    "        \"\"\"Save prediction results to JSON file.\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "    def extract_domain(self, email_or_domain):\n",
    "        \"\"\"Extract domain from email.\"\"\"\n",
    "        if not email_or_domain:\n",
    "            return \"\"\n",
    "        email_or_domain = str(email_or_domain).strip().lower()\n",
    "        if '@' in email_or_domain:\n",
    "            return email_or_domain.split('@')[-1]\n",
    "        return email_or_domain\n",
    "\n",
    "    def extract_features(self, email_or_domain):\n",
    "        \"\"\"Extract features using  techniques.\"\"\"\n",
    "        domain = self.extract_domain(email_or_domain)\n",
    "\n",
    "        # Check cache first\n",
    "        cache_key = f\"features_{domain}\"\n",
    "        if cache_key in self.feature_cache:\n",
    "            return self.feature_cache[cache_key]\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        # 1. Domain structure features\n",
    "        features.update(self._extract_structure_features(domain))\n",
    "\n",
    "        # 2. Statistical features\n",
    "        features.update(self._extract_statistical_features(domain))\n",
    "\n",
    "        # 3. Pattern matching features\n",
    "        features.update(self._extract_pattern_features(domain))\n",
    "\n",
    "        # 4. DNS features (if enabled)\n",
    "        if self.config.get('use_dns', True):\n",
    "            features.update(self._extract_dns_features(domain))\n",
    "\n",
    "        # 5. Similarity features\n",
    "        features.update(self._extract_similarity_features(domain))\n",
    "\n",
    "        # 6. List membership (encoded properly)\n",
    "        features['in_disposable_list'] = 1.0 if domain in self.disposable_domains else 0.0\n",
    "        features['in_legitimate_list'] = 1.0 if domain in self.legitimate_domains else 0.0\n",
    "        \n",
    "        # NEW: Add feature for suspected domains\n",
    "        features['in_suspected_list'] = 1.0 if domain in self.suspected_disposable_domains else 0.0\n",
    "\n",
    "        # Cache the features\n",
    "        self.feature_cache[cache_key] = features\n",
    "        return features\n",
    "\n",
    "    def _extract_structure_features(self, domain):\n",
    "        \"\"\"Extract structural features from domain.\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Basic structure\n",
    "        parts = domain.split('.')\n",
    "        features['domain_length'] = len(domain)\n",
    "        features['subdomain_count'] = len(parts) - 2 if len(parts) > 2 else 0\n",
    "        features['tld_length'] = len(parts[-1]) if parts else 0\n",
    "\n",
    "        # Character composition\n",
    "        features['digit_ratio'] = sum(c.isdigit() for c in domain) / len(domain) if domain else 0\n",
    "        features['alpha_ratio'] = sum(c.isalpha() for c in domain) / len(domain) if domain else 0\n",
    "        features['special_char_ratio'] = sum(not c.isalnum() and c != '.' for c in domain) / len(domain) if domain else 0\n",
    "\n",
    "        # Entropy (randomness measure)\n",
    "        features['domain_entropy'] = self._calculate_entropy(domain)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_statistical_features(self, domain):\n",
    "        \"\"\"Extract statistical features.\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Character distribution\n",
    "        char_counts = {}\n",
    "        for char in domain:\n",
    "            char_counts[char] = char_counts.get(char, 0) + 1\n",
    "\n",
    "        # Statistical measures\n",
    "        counts = list(char_counts.values())\n",
    "        features['char_mean_frequency'] = np.mean(counts) if counts else 0\n",
    "        features['char_std_frequency'] = np.std(counts) if counts else 0\n",
    "        features['char_unique_ratio'] = len(char_counts) / len(domain) if domain else 0\n",
    "\n",
    "        # Vowel/consonant ratio\n",
    "        vowels = set('aeiouAEIOU')\n",
    "        vowel_count = sum(1 for c in domain if c in vowels)\n",
    "        consonant_count = sum(1 for c in domain if c.isalpha() and c not in vowels)\n",
    "        features['vowel_consonant_ratio'] = vowel_count / (consonant_count + 1)\n",
    "\n",
    "        # Sequential patterns\n",
    "        features['max_char_sequence'] = self._max_char_sequence(domain)\n",
    "        features['digit_sequences'] = len(re.findall(r'\\d+', domain))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_pattern_features(self, domain):\n",
    "        \"\"\"Extract pattern-based features.\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Disposable patterns\n",
    "        disposable_score = 0\n",
    "        for keyword in self.disposable_patterns['keywords']:\n",
    "            if keyword in domain:\n",
    "                disposable_score += 1\n",
    "        for pattern in self.disposable_patterns['patterns']:\n",
    "            if re.search(pattern, domain):\n",
    "                disposable_score += 1\n",
    "        features['disposable_pattern_score'] = disposable_score\n",
    "\n",
    "        # Legitimate patterns\n",
    "        legitimate_score = 0\n",
    "        for tld in self.legitimate_patterns['tlds']:\n",
    "            if domain.endswith(tld):\n",
    "                legitimate_score += 1\n",
    "        for pattern in self.legitimate_patterns['patterns']:\n",
    "            if re.match(pattern, domain):\n",
    "                legitimate_score += 1\n",
    "        features['legitimate_pattern_score'] = legitimate_score\n",
    "\n",
    "        # Specific patterns\n",
    "        features['has_numbers'] = float(bool(re.search(r'\\d', domain)))\n",
    "        features['starts_with_digit'] = float(domain[0].isdigit() if domain else False)\n",
    "        features['has_hyphen'] = float('-' in domain)\n",
    "        features['multiple_dots'] = float(domain.count('.') > 1)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_dns_features(self, domain):\n",
    "        \"\"\"Extract DNS-based features.\"\"\"\n",
    "        features = {\n",
    "            'has_mx_record': 0.0,\n",
    "            'has_a_record': 0.0,\n",
    "            'mx_count': 0,\n",
    "            'a_count': 0\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Check MX records\n",
    "            mx_records = dns.resolver.resolve(domain, 'MX', lifetime=2)\n",
    "            features['has_mx_record'] = 1.0\n",
    "            features['mx_count'] = len(mx_records)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            # Check A records\n",
    "            a_records = dns.resolver.resolve(domain, 'A', lifetime=2)\n",
    "            features['has_a_record'] = 1.0\n",
    "            features['a_count'] = len(a_records)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_similarity_features(self, domain):\n",
    "        \"\"\"Extract similarity features using  distance metrics.\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Sample domains for comparison\n",
    "        legitimate_sample = list(self.legitimate_domains)[:50]\n",
    "        disposable_sample = list(self.disposable_domains)[:50]\n",
    "\n",
    "        # Calculate similarities\n",
    "        if legitimate_sample:\n",
    "            similarities = [self._calculate_similarity(domain, ref) for ref in legitimate_sample]\n",
    "            features['max_legitimate_similarity'] = max(similarities)\n",
    "            features['avg_legitimate_similarity'] = np.mean(similarities)\n",
    "            features['std_legitimate_similarity'] = np.std(similarities)\n",
    "        else:\n",
    "            features['max_legitimate_similarity'] = 0\n",
    "            features['avg_legitimate_similarity'] = 0\n",
    "            features['std_legitimate_similarity'] = 0\n",
    "\n",
    "        if disposable_sample:\n",
    "            similarities = [self._calculate_similarity(domain, ref) for ref in disposable_sample]\n",
    "            features['max_disposable_similarity'] = max(similarities)\n",
    "            features['avg_disposable_similarity'] = np.mean(similarities)\n",
    "        else:\n",
    "            features['max_disposable_similarity'] = 0\n",
    "            features['avg_disposable_similarity'] = 0\n",
    "\n",
    "        # Relative similarity score\n",
    "        features['similarity_differential'] = (\n",
    "            features['avg_legitimate_similarity'] - features['avg_disposable_similarity']\n",
    "        )\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _calculate_entropy(self, text):\n",
    "        \"\"\"Calculate Shannon entropy of text.\"\"\"\n",
    "        if not text:\n",
    "            return 0\n",
    "\n",
    "        # Character frequency distribution\n",
    "        char_freq = {}\n",
    "        for char in text:\n",
    "            char_freq[char] = char_freq.get(char, 0) + 1\n",
    "\n",
    "        # Calculate entropy\n",
    "        entropy = 0\n",
    "        for count in char_freq.values():\n",
    "            probability = count / len(text)\n",
    "            if probability > 0:\n",
    "                entropy -= probability * math.log2(probability)\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def _max_char_sequence(self, text):\n",
    "        \"\"\"Find maximum consecutive character sequence.\"\"\"\n",
    "        if not text:\n",
    "            return 0\n",
    "\n",
    "        max_seq = 1\n",
    "        current_seq = 1\n",
    "\n",
    "        for i in range(1, len(text)):\n",
    "            if text[i] == text[i-1]:\n",
    "                current_seq += 1\n",
    "                max_seq = max(max_seq, current_seq)\n",
    "            else:\n",
    "                current_seq = 1\n",
    "\n",
    "        return max_seq\n",
    "\n",
    "    def _calculate_similarity(self, domain1, domain2):\n",
    "        \"\"\"Calculate similarity using multiple metrics.\"\"\"\n",
    "        # Levenshtein distance (normalized)\n",
    "        levenshtein = difflib.SequenceMatcher(None, domain1, domain2).ratio()\n",
    "\n",
    "        # Jaccard similarity (character-level)\n",
    "        set1 = set(domain1)\n",
    "        set2 = set(domain2)\n",
    "        jaccard = len(set1.intersection(set2)) / len(set1.union(set2)) if set1.union(set2) else 0\n",
    "\n",
    "        # Combined similarity\n",
    "        return (levenshtein + jaccard) / 2\n",
    "\n",
    "    def train(self, force_retrain=False):\n",
    "        \"\"\"Train the model with proper  foundations.\"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"TRAINING  EMAIL DETECTOR\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # Check if model exists and not forcing retrain\n",
    "        if not force_retrain and os.path.exists(self.model_path):\n",
    "            print(\"Model already exists. Use force_retrain=True to retrain.\")\n",
    "            return self.load_model()\n",
    "\n",
    "        # Prepare balanced training data\n",
    "        print(\"\\n1. Preparing balanced training data...\")\n",
    "        X, y = self._prepare_training_data()\n",
    "\n",
    "        # Feature scaling\n",
    "        print(\"\\n2. Scaling features...\")\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        X_scaled = self.feature_scaler.fit_transform(X)\n",
    "\n",
    "        # Feature selection\n",
    "        print(\"\\n3. Selecting best features...\")\n",
    "        self.feature_selector = SelectKBest(\n",
    "            score_func=mutual_info_classif,\n",
    "            k=min(self.config.get('max_features', 30), X.shape[1])\n",
    "        )\n",
    "        X_selected = self.feature_selector.fit_transform(X_scaled, y)\n",
    "\n",
    "        # Get selected feature names\n",
    "        selected_indices = self.feature_selector.get_support(indices=True)\n",
    "        selected_features = [X.columns[i] for i in selected_indices]\n",
    "        print(f\"Selected {len(selected_features)} features\")\n",
    "\n",
    "        # Class weights for balanced training\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y),\n",
    "            y=y\n",
    "        )\n",
    "        class_weight_dict = dict(zip(np.unique(y), class_weights))\n",
    "\n",
    "        # Train ensemble model\n",
    "        print(\"\\n4. Training ensemble model...\")\n",
    "        base_model = self._create_ensemble_model(class_weight_dict)\n",
    "\n",
    "        # Cross-validation\n",
    "        cv = StratifiedKFold(n_splits=self.config.get('cv_folds', 5), shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(base_model, X_selected, y, cv=cv, scoring='roc_auc')\n",
    "        print(f\"Cross-validation AUC: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "\n",
    "        # Train final model\n",
    "        base_model.fit(X_selected, y)\n",
    "\n",
    "        # Calibrate probabilities\n",
    "        print(\"\\n5. Calibrating probabilities...\")\n",
    "        self.calibrated_model = CalibratedClassifierCV(\n",
    "            base_model,\n",
    "            method='sigmoid',\n",
    "            cv=3\n",
    "        )\n",
    "        self.calibrated_model.fit(X_selected, y)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        print(\"\\n6. Evaluating model...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_selected, y, test_size=0.2, stratify=y, random_state=42\n",
    "        )\n",
    "\n",
    "        y_pred = self.calibrated_model.predict(X_test)\n",
    "        y_proba = self.calibrated_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "        print(f\"Test Precision: {precision_score(y_test, y_pred):.3f}\")\n",
    "        print(f\"Test Recall: {recall_score(y_test, y_pred):.3f}\")\n",
    "        print(f\"Test F1: {f1_score(y_test, y_pred):.3f}\")\n",
    "        print(f\"Test AUC: {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "\n",
    "        # Store model configuration\n",
    "        self.model_config = {\n",
    "            'feature_names': list(X.columns),\n",
    "            'selected_features': selected_features,\n",
    "            'feature_scaler': self.feature_scaler,\n",
    "            'feature_selector': self.feature_selector,\n",
    "            'training_date': datetime.now().isoformat(),\n",
    "            'n_samples': len(X),\n",
    "            'n_features': len(selected_features),\n",
    "            'class_balance': dict(zip(*np.unique(y, return_counts=True)))\n",
    "        }\n",
    "\n",
    "        # Save model\n",
    "        self.save_model()\n",
    "        print(\"\\n7. Model saved successfully!\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _prepare_training_data(self):\n",
    "        \"\"\"Prepare balanced training data.\"\"\"\n",
    "        features_list = []\n",
    "        labels = []\n",
    "\n",
    "        # Extract features for legitimate domains\n",
    "        for domain in tqdm(self.legitimate_domains, desc=\"Processing legitimate domains\"):\n",
    "            features = self.extract_features(domain)\n",
    "            features_list.append(features)\n",
    "            labels.append(0)\n",
    "\n",
    "        # Extract features for disposable domains\n",
    "        for domain in tqdm(self.disposable_domains, desc=\"Processing disposable domains\"):\n",
    "            features = self.extract_features(domain)\n",
    "            features_list.append(features)\n",
    "            labels.append(1)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        X = pd.DataFrame(features_list)\n",
    "        y = np.array(labels)\n",
    "\n",
    "        # Balance the dataset if needed\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"\\nClass distribution: {dict(zip(unique, counts))}\")\n",
    "\n",
    "        # If imbalanced, undersample the majority class\n",
    "        if counts[0] != counts[1]:\n",
    "            min_count = min(counts)\n",
    "            balanced_indices = []\n",
    "\n",
    "            for label in unique:\n",
    "                label_indices = np.where(y == label)[0]\n",
    "                sampled_indices = np.random.choice(label_indices, min_count, replace=False)\n",
    "                balanced_indices.extend(sampled_indices)\n",
    "\n",
    "            np.random.shuffle(balanced_indices)\n",
    "            X = X.iloc[balanced_indices]\n",
    "            y = y[balanced_indices]\n",
    "\n",
    "            print(f\"Balanced to {min_count} samples per class\")\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def _create_ensemble_model(self, class_weights):\n",
    "        \"\"\"Create ensemble model with proper configuration.\"\"\"\n",
    "        # Use a calibrated ensemble\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            class_weight=class_weights,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        gb = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        lr = LogisticRegression(\n",
    "            class_weight=class_weights,\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Voting classifier\n",
    "        from sklearn.ensemble import VotingClassifier\n",
    "        ensemble = VotingClassifier(\n",
    "            estimators=[('rf', rf), ('gb', gb), ('lr', lr)],\n",
    "            voting='soft',\n",
    "            weights=[0.4, 0.4, 0.2]  # Give more weight to tree-based models\n",
    "        )\n",
    "\n",
    "        return ensemble\n",
    "\n",
    "    def predict(self, emails_or_domains):\n",
    "        \"\"\"Make predictions with  confidence scores.\"\"\"\n",
    "        if isinstance(emails_or_domains, str):\n",
    "            emails_or_domains = [emails_or_domains]\n",
    "\n",
    "        # Check if model is loaded\n",
    "        if self.calibrated_model is None:\n",
    "            if not self.load_model():\n",
    "                print(\"No trained model found. Please train first.\")\n",
    "                return []\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for email_or_domain in tqdm(emails_or_domains, desc=\"Processing emails\"):\n",
    "            domain = self.extract_domain(email_or_domain)\n",
    "\n",
    "            # Extract features\n",
    "            features = self.extract_features(email_or_domain)\n",
    "            features_df = pd.DataFrame([features])\n",
    "\n",
    "            # Apply same preprocessing as training\n",
    "            features_scaled = self.feature_scaler.transform(features_df)\n",
    "            features_selected = self.feature_selector.transform(features_scaled)\n",
    "\n",
    "            # Make prediction\n",
    "            prediction = self.calibrated_model.predict(features_selected)[0]\n",
    "            probabilities = self.calibrated_model.predict_proba(features_selected)[0]\n",
    "\n",
    "            # Get confidence (calibrated probability)\n",
    "            confidence = probabilities[prediction]\n",
    "\n",
    "            # Get feature contributions\n",
    "            contributions = self._get_feature_contributions(features, features_df)\n",
    "            \n",
    "            # Store for potential use in suspected domain saving\n",
    "            self.last_top_features = contributions[:5]\n",
    "\n",
    "            result = {\n",
    "                'input': email_or_domain,\n",
    "                'domain': domain,\n",
    "                'is_disposable': bool(prediction),\n",
    "                'confidence': float(confidence),\n",
    "                'probability_legitimate': float(probabilities[0]),\n",
    "                'probability_disposable': float(probabilities[1]),\n",
    "                'top_features': contributions[:5],\n",
    "                'in_training_legitimate': domain in self.legitimate_domains,\n",
    "                'in_training_disposable': domain in self.disposable_domains,\n",
    "                'in_suspected_disposable': domain in self.suspected_disposable_domains\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "            \n",
    "            # NEW: Check if we should save this as a suspected disposable domain\n",
    "            self._check_and_save_suspected_domain(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _check_and_save_suspected_domain(self, result):\n",
    "        \"\"\"Check if a domain should be saved as a suspected disposable domain.\"\"\"\n",
    "        domain = result['domain']\n",
    "        \n",
    "        # Skip if already in training data or suspected list\n",
    "        if (result['in_training_legitimate'] or \n",
    "            result['in_training_disposable'] or \n",
    "            result['in_suspected_disposable']):\n",
    "            return\n",
    "        \n",
    "        # Check if it's disposable with high confidence\n",
    "        if (result['is_disposable'] and \n",
    "            result['confidence'] >= self.suspected_domains_threshold):\n",
    "            \n",
    "            # Prepare reasons\n",
    "            reasons = []\n",
    "            \n",
    "            # Add confidence level\n",
    "            reasons.append(f\"High confidence ({result['confidence']:.2%})\")\n",
    "            \n",
    "            # Add top contributing features\n",
    "            for feature in result['top_features'][:3]:\n",
    "                if feature['contribution'] > 0.1:  # Only significant contributions\n",
    "                    reasons.append(f\"{feature['feature']}={feature['value']:.3f}\")\n",
    "            \n",
    "            # Add specific pattern matches if any\n",
    "            features = self.feature_cache.get(f\"features_{domain}\", {})\n",
    "            if features.get('disposable_pattern_score', 0) > 0:\n",
    "                reasons.append(f\"Disposable patterns detected\")\n",
    "            if features.get('domain_entropy', 0) > 3.5:\n",
    "                reasons.append(f\"High entropy ({features['domain_entropy']:.2f})\")\n",
    "            if features.get('digit_ratio', 0) > 0.3:\n",
    "                reasons.append(f\"High digit ratio ({features['digit_ratio']:.2f})\")\n",
    "            \n",
    "            # Save the suspected domain\n",
    "            self._save_suspected_domain(domain, result['confidence'], reasons)\n",
    "\n",
    "    def _get_feature_contributions(self, features, features_df):\n",
    "        \"\"\"Get feature contributions using information gain.\"\"\"\n",
    "        if not hasattr(self, 'feature_selector'):\n",
    "            return []\n",
    "\n",
    "        # Get feature scores from the selector\n",
    "        feature_scores = self.feature_selector.scores_\n",
    "        feature_names = features_df.columns\n",
    "\n",
    "        # Get selected features and their scores\n",
    "        selected_mask = self.feature_selector.get_support()\n",
    "        selected_features = []\n",
    "\n",
    "        for i, (name, selected) in enumerate(zip(feature_names, selected_mask)):\n",
    "            if selected:\n",
    "                score = feature_scores[i]\n",
    "                value = features.get(name, 0)\n",
    "                selected_features.append({\n",
    "                    'feature': name,\n",
    "                    'value': value,\n",
    "                    'importance': score,\n",
    "                    'contribution': score * abs(value)\n",
    "                })\n",
    "\n",
    "        # Sort by contribution\n",
    "        selected_features.sort(key=lambda x: x['contribution'], reverse=True)\n",
    "\n",
    "        return selected_features\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"Save the trained model and configuration.\"\"\"\n",
    "        model_data = {\n",
    "            'calibrated_model': self.calibrated_model,\n",
    "            'feature_scaler': self.feature_scaler,\n",
    "            'feature_selector': self.feature_selector,\n",
    "            'model_config': self.model_config,\n",
    "            'version': '4.2' \n",
    "        }\n",
    "\n",
    "        with open(self.model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the trained model and configuration.\"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            self.logger.warning(\"No saved model found.\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            with open(self.model_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "\n",
    "            self.calibrated_model = model_data['calibrated_model']\n",
    "            self.feature_scaler = model_data['feature_scaler']\n",
    "            self.feature_selector = model_data['feature_selector']\n",
    "            self.model_config = model_data['model_config']\n",
    "\n",
    "            self.logger.info(\"Model loaded successfully.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading model: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def explain_prediction(self, email_or_domain):\n",
    "        \"\"\"Provide a detailed explanation of the prediction.\"\"\"\n",
    "        results = self.predict(email_or_domain)\n",
    "        if not results:\n",
    "            return \"No prediction available\"\n",
    "\n",
    "        result = results[0]\n",
    "        domain = result['domain']\n",
    "\n",
    "        explanation = f\"\"\"\n",
    "Prediction Explanation for: {email_or_domain}\n",
    "{'='*50}\n",
    "\n",
    "Domain: {domain}\n",
    "Prediction: {'DISPOSABLE' if result['is_disposable'] else 'LEGITIMATE'}\n",
    "Confidence: {result['confidence']:.1%}\n",
    "\n",
    "Probability Distribution:\n",
    "- Legitimate: {result['probability_legitimate']:.1%}\n",
    "- Disposable: {result['probability_disposable']:.1%}\n",
    "\n",
    "Training Data Status:\n",
    "- In Legitimate List: {result['in_training_legitimate']}\n",
    "- In Disposable List: {result['in_training_disposable']}\n",
    "- In Suspected Disposable List: {result['in_suspected_disposable']}\n",
    "\n",
    "Top Contributing Features:\n",
    "\"\"\"\n",
    "\n",
    "        for i, feature in enumerate(result['top_features'], 1):\n",
    "            explanation += f\"\\n{i}. {feature['feature']}\"\n",
    "            explanation += f\"\\n   Value: {feature['value']:.3f}\"\n",
    "            explanation += f\"\\n   Importance: {feature['importance']:.3f}\"\n",
    "            explanation += f\"\\n   Contribution: {feature['contribution']:.3f}\"\n",
    "\n",
    "        # Feature analysis\n",
    "        features = self.extract_features(email_or_domain)\n",
    "\n",
    "        explanation += f\"\\n\\nFeature Analysis:\"\n",
    "        explanation += f\"\\n- Domain Entropy: {features.get('domain_entropy', 0):.3f}\"\n",
    "        explanation += f\"\\n- Character Diversity: {features.get('char_unique_ratio', 0):.3f}\"\n",
    "        explanation += f\"\\n- Legitimate Similarity: {features.get('avg_legitimate_similarity', 0):.3f}\"\n",
    "        explanation += f\"\\n- Disposable Similarity: {features.get('avg_disposable_similarity', 0):.3f}\"\n",
    "\n",
    "        return explanation\n",
    "\n",
    "    def review_suspected_domains(self, output_path=None):\n",
    "        \"\"\"Review all suspected disposable domains collected so far.\"\"\"\n",
    "        if not os.path.exists(self.suspected_disposable_path):\n",
    "            print(\"No suspected domains file found.\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(self.suspected_disposable_path)\n",
    "        print(f\"\\nSuspected Disposable Domains Report\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total suspected domains: {len(df)}\")\n",
    "        print(f\"\\nMost recent 10 domains:\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        # Sort by timestamp descending\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df_sorted = df.sort_values('timestamp', ascending=False)\n",
    "        \n",
    "        for idx, row in df_sorted.head(10).iterrows():\n",
    "            print(f\"\\nDomain: {row['domain']}\")\n",
    "            print(f\"Confidence: {row['confidence']:.3f}\")\n",
    "            print(f\"Detected: {row['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"Reasons: {row['reasons']}\")\n",
    "        \n",
    "        if output_path:\n",
    "            df_sorted.to_csv(output_path, index=False)\n",
    "            print(f\"\\nFull report saved to: {output_path}\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\nSummary Statistics:\")\n",
    "        print(f\"Average confidence: {df['confidence'].mean():.3f}\")\n",
    "        print(f\"Min confidence: {df['confidence'].min():.3f}\")\n",
    "        print(f\"Max confidence: {df['confidence'].max():.3f}\")\n",
    "        \n",
    "        # Most common features\n",
    "        if 'top_features' in df.columns:\n",
    "            all_features = []\n",
    "            for features_str in df['top_features']:\n",
    "                try:\n",
    "                    features = json.loads(features_str)\n",
    "                    all_features.extend(features)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if all_features:\n",
    "                from collections import Counter\n",
    "                feature_counts = Counter(all_features)\n",
    "                print(f\"\\nMost common features in suspected domains:\")\n",
    "                for feature, count in feature_counts.most_common(5):\n",
    "                    print(f\"- {feature}: {count} occurrences\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Demonstration of the  email detector.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\" EMAIL DETECTOR WITH FILE INPUT\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Initialize detector\n",
    "    detector = EmailDetector()\n",
    "\n",
    "    # Train model (if needed)\n",
    "    if not os.path.exists(detector.model_path):\n",
    "        print(\"\\nTraining new model...\")\n",
    "        detector.train()\n",
    "    else:\n",
    "        print(\"\\nLoading existing model...\")\n",
    "        detector.load_model()\n",
    "\n",
    "    # Example 1: Test with predefined emails\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: INDIVIDUAL EMAIL TESTING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_emails = [\n",
    "        'q5qa2s2@mydefipet.live',\n",
    "        'sicerol897@hazhab.com',\n",
    "        'pylasato@cyclelove.cc',\n",
    "        'hey@e.platypusshoes.co.nz',\n",
    "        'monday.reply@brighttalk.com',\n",
    "    ]\n",
    "\n",
    "    results = detector.predict(test_emails)\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        status = \"DISPOSABLE\" if result['is_disposable'] else \"LEGITIMATE\"\n",
    "        print(f\"\\n{i}. Email: {result['input']}\")\n",
    "        print(f\"   Status: {status}\")\n",
    "        print(f\"   Confidence: {result['confidence']:.1%}\")\n",
    "\n",
    "    # Example 2: Test with file input\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: FILE INPUT TESTING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create a sample file if it doesn't exist\n",
    "    sample_file = \"input/test_emails.txt\"\n",
    "    if not os.path.exists(sample_file):\n",
    "        print(f\"\\nCreating sample file: {sample_file}\")\n",
    "        with open(sample_file, 'w') as f:\n",
    "            f.write(\"# Test emails for detection\\n\")\n",
    "            f.write(\"test123@tempmail.com\\n\")\n",
    "    \n",
    "    # Process emails from file\n",
    "    results = detector.check_emails_from_file(\n",
    "        sample_file,\n",
    "        output_csv=\"/output/email_check_results.csv\",\n",
    "        output_json=\"/output/email_check_results.json\"\n",
    "    )\n",
    "\n",
    "    # Example 3: Review suspected domains\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: SUSPECTED DOMAINS REVIEW\")\n",
    "    print(\"=\"*70)\n",
    "    detector.review_suspected_domains()\n",
    "\n",
    "    # Example 4: Batch processing from large file\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 4: BATCH PROCESSING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create a larger test file\n",
    "    batch_file = \"/input/batch_emails.txt\"\n",
    "    if not os.path.exists(batch_file):\n",
    "        print(f\"\\nCreating batch file: {batch_file}\")\n",
    "        with open(batch_file, 'w') as f:\n",
    "            f.write(\"# Batch email processing example\\n\")\n",
    "            # Add various test emails\n",
    "            test_domains = [\n",
    "                \"gmail.com\", \"tempmail.com\", \"guerrillamail.com\", \n",
    "                \"outlook.com\", \"mailinator.com\", \"example.com\"\n",
    "            ]\n",
    "            for i in range(20):\n",
    "                domain = test_domains[i % len(test_domains)]\n",
    "                f.write(f\"user{i}@{domain}\\n\")\n",
    "    \n",
    "    # Process batch file\n",
    "    detector.check_emails_from_file(batch_file)\n",
    "\n",
    "    # Example 5: Detailed explanation\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 5: DETAILED EXPLANATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    explanation = detector.explain_prediction('test123@tempmail456.com')\n",
    "    print(explanation)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
